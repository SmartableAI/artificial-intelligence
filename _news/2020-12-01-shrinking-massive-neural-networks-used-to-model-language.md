---
category: news
title: "Shrinking massive neural networks used to model language"
excerpt: "Deep learning neural networks can be massive, demanding major computing power. In a test of the “lottery ticket hypothesis,” MIT researchers have found leaner, more efficient subnetworks hidden within BERT models."
publishedDateTime: 2020-12-01T05:28:00Z
originalUrl: "https://news.mit.edu/2020/neural-model-language-1201"
webUrl: "https://news.mit.edu/2020/neural-model-language-1201"
type: article
quality: 69
heat: 69
published: false

provider:
  name: Massachusetts Institute of Technology
  domain: mit.edu
  images:
    - url: "https://smartableai.github.io/artificial-intelligence/assets/images/organizations/mit.edu-50x50.jpg"
      width: 50
      height: 50

topics:
  - Machine Learning
  - AI

images:
  - url: "https://news.mit.edu/sites/default/files/images/202011/MIT-BERT-Lottery-01-Press.jpg"
    width: 1500
    height: 1000
    isCached: true

secured: "HxXeWEVf9Abf4LWfBStjqhqdJTgtLIf0OiCTPeQi4Y5I378M6Y1TzYO3V8vfc+POKka8knXQShvPz8Ko30zHZNSuGy9PfaRuBvOP2WSKl0ogELykOje8xOpC5QcUqYjo8fn00dux8jOHYe0qv/43Za2xzouRndDJIh4MritW9GdF98KEV2uEN0CiVmEwjniy3sQk0WGNEswU0dcX+ktVaT2seRyx9QHBYmSSoTlFsaEZX4vnGj3J46tuZBOsTmoZvJIxR9Lz3q9LVpcuPQWnSIW5ZUssBfx82seHBWVv3EuA+4jSMc48/nLBjpbgEVipj+DiIBNdMKk2T3D6Vq71TaMB6tKJHxuDq2sgsCqUvao=;ILJUm0aBk/uhAFndDXqbEg=="
---

