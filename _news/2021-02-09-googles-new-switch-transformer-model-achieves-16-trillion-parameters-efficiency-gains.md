---
category: news
title: "Googleâ€™s New Switch Transformer Model Achieves 1.6 Trillion Parameters, Efficiency Gains"
excerpt: "OpenAI wowed the world with its eerily human language generator, GPT-3. The autoregressive model stood at a then-staggering 175 billion"
publishedDateTime: 2021-02-08T17:36:00Z
originalUrl: "https://www.datanami.com/2021/02/08/googles-new-switch-transformer-model-achieves-1-6-trillion-parameters-efficiency-gains/"
webUrl: "https://www.datanami.com/2021/02/08/googles-new-switch-transformer-model-achieves-1-6-trillion-parameters-efficiency-gains/"
type: article
quality: 30
heat: 30
published: false

provider:
  name: datanami.com
  domain: datanami.com

topics:
  - Natural Language Processing
  - AI

images:
  - url: "https://2s7gjr373w3x22jf92z99mgm5w-wpengine.netdna-ssl.com/wp-content/uploads/2018/07/google_shutterstock_achinthamb.jpg"
    width: 1000
    height: 667
    isCached: true

secured: "OwCg9NhGpvnLSRYrER16MFcY/JZqaL7jrvKe35dVDLBXAKqkYEh7Bj/RWvcy/FKYj9dUW0HAd7awDH/j4a4n0L3Ct1i4f5b2Ci0ogimPXMQ8GNgYn/i+fDMVze5DO3oAEsdhNmk0Wg75P/FfygF25ArVU+6kDkRvITcigXmPQPXOGu/+F3kkp+BLZ2GfLRuYVHRIBgc7JCeKVS0Awn3joBmJfQmBOupTzxnUlX5+TL9j3/irmP5AT1xTaFXJT7I4HakRXNTWKXJkzo4ZylgOjbC9o/Y1T4dAFIhDmG9Xk7bIM94FP8KCuYYSnjjXGoH/pXPdK9GgFJB0uHeOn7YLMasY+TfR+l8CWdCYh8NAYhZUiwv8nyr/EsFJsQlssh+faGz8LjcnFx2vM55f5jzAFljx0BWOO+M1QLUtan4DGlnBQDR+xZN9pav+1j5KIZDzcrV3TP5CbjXDKx5bW6KiYdZjrRdAJ9NDE6eTCLPohswdtuZhybq9pZC30gp8fLHL3YBZIqIgHvIXaH9VnQqJkA==;CWWw64K9rhtrsGq/b2YDrw=="
---

