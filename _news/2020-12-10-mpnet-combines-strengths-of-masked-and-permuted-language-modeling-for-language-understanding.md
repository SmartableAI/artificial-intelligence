---
category: news
title: "MPNet combines strengths of masked and permuted language modeling for language understanding"
excerpt: "Pretrained language models have been a hot research topic in natural language processing. These models, such as BERT, are usually pretrained on large-scale language corpora with carefully designed pretraining objectives and then fine-tuned on downstream tasks to boost the accuracy."
publishedDateTime: 2020-12-09T19:22:00Z
originalUrl: "https://www.microsoft.com/en-us/research/blog/mpnet-combines-strengths-of-masked-and-permuted-language-modeling-for-language-understanding/"
webUrl: "https://www.microsoft.com/en-us/research/blog/mpnet-combines-strengths-of-masked-and-permuted-language-modeling-for-language-understanding/"
type: article
quality: 13
heat: 13
published: false

provider:
  name: Microsoft
  domain: microsoft.com
  images:
    - url: "https://smartableai.github.io/artificial-intelligence/assets/images/organizations/microsoft.com-50x50.jpg"
      width: 50
      height: 50

topics:
  - Natural Language Processing
  - AI

images:
  - url: "https://www.microsoft.com/en-us/research/uploads/prod/2020/12/1400x788_mpnet_no_logo_still-scaled.jpg"
    width: 2560
    height: 1442
    isCached: true

secured: "8urSSbnaF99ylYB5fuGgh5uPw5CnRxG9a0AJmm79NHRcrmLl8t0uFvyFsY2K9j4KItHMAb3sXPA1WHnS1hNIP553A9Eby7taiwz9Xo+yrcImaQ9lbcIekVdCHBrJ94QwRYI+nfrYXoZZZQWBqBQaKkt+0t5J8VSqR+lLBEsI6abqYTh0AeqskOzOB0lnqqOvqElXL51zy9c6gUmUidcZ8m7wDmZlbgNeH+VCCVVupe1LsXaoa6aMyA0E/VBY20qy0FguHQS5R0yekv0hulocAX03pxratMfSujuQGTlmmc/50iKzFHjuT9Zawh7OXy50skbibV6PhBaNUQ8Gey6FlX8UYS4uPI3maltJcG0doVw=;cRhzQ0GnakuBECXShf9T7Q=="
---

