---
category: news
title: "Googleâ€™s New Switch Transformer Model Achieves 1.6 Trillion Parameters, Efficiency Gains"
excerpt: "OpenAI wowed the world with its eerily human language generator, GPT-3. The autoregressive model stood at a then-staggering 175 billion"
publishedDateTime: 2021-02-08T17:36:00Z
originalUrl: "https://www.datanami.com/2021/02/08/googles-new-switch-transformer-model-achieves-1-6-trillion-parameters-efficiency-gains/"
webUrl: "https://www.datanami.com/2021/02/08/googles-new-switch-transformer-model-achieves-1-6-trillion-parameters-efficiency-gains/"
type: article
quality: 30
heat: 30
published: false

provider:
  name: datanami.com
  domain: datanami.com

topics:
  - Natural Language Processing
  - AI

images:
  - url: "https://2s7gjr373w3x22jf92z99mgm5w-wpengine.netdna-ssl.com/wp-content/uploads/2018/07/google_shutterstock_achinthamb.jpg"
    width: 1000
    height: 667
    isCached: true

secured: "TgrCHHAQhZjP5H58Q+Th6XKcbTcB3dx3fYqXfSDzcQldZCbkHnOQSyBscQXFZg2eHDeVQfT2QGEZY8+bXlarZzMx9ErhfQJFIiYggHLVz5Ene2RrNABnieaedFdhZiDG8hpripidAFZhISmqqDwuJjks11eVwXcD3h2DqYn5rz8yWbngcKrNnT4LkYYku8Ku93ZaEOp3InbcCuW2AFxWzRpRsR0OjtGMeBOFJurutTHoYRUPjXa8E8bl4ftGvPP0lixeZxPF+1JSBC/zQdM/HlLRDFRDIYrdS1wgKvLh/LecDMawagie5yRgEKfbLyQbjBQSejCQIKqiquLzN4//F3S88APpZ7rQIW3X89q0xs0=;bLftEwijdQ3FnugSS5RyLA=="
---

