---
category: news
title: "Shrinking massive neural networks used to model language"
excerpt: "Deep learning neural networks can be massive, demanding major computing power. In a test of the ''lottery ticket hypothesis,'' MIT researchers have found leaner, more efficient subnetworks hidden within BERT models."
publishedDateTime: 2020-12-01T18:34:00Z
originalUrl: "https://www.eurekalert.org/pub_releases/2020-12/miot-smn120120.php"
webUrl: "https://www.eurekalert.org/pub_releases/2020-12/miot-smn120120.php"
type: article
quality: 24
heat: -1
published: false

provider:
  name: EurekAlert!
  domain: eurekalert.org

topics:
  - Natural Language Processing
  - AI

images:
  - url: "https://www.eurekalert.org/socialshare/EurekAlert-bluebg_Twitter_1200x675.jpg"
    width: 1200
    height: 675
    isCached: true

related:
  - title: "Shrinking massive neural networks used to model language"
    excerpt: "Deep learning neural networks can be massive, demanding major computing power. In a test of the “lottery ticket hypothesis,” MIT researchers have found leaner, more efficient subnetworks hidden within BERT models."
    publishedDateTime: 2020-12-01T05:28:00Z
    webUrl: "https://news.mit.edu/2020/neural-model-language-1201"
    type: article
    provider:
      name: Massachusetts Institute of Technology
      domain: mit.edu
    quality: 71
    images:
      - url: "https://news.mit.edu/sites/default/files/images/202011/MIT-BERT-Lottery-01-Press.jpg"
        width: 1500
        height: 1000
        isCached: true

secured: "mCfAtCEjt9U2MlmWbka57oZD9WbnO9BNOvQB9g0nRlo7pcqMLBLM9fhbB+LTBVjCINTMYSQfohGucx68NIomU3n9fxlCnsx9+ypFix+iVDGFxPNPx/CfC07PaWvEyD8A6x7P41KsgmHG4/aCqrx2tIQeLNV4gRiAnPSp6Qvekh9mzIlGNVZZwkeSm6S1ey4KhFG1F+psidET/QBfoP0l1SUfzUYHJZdFnP8UGzPFkLuVowQj0bVWLceom4oaYe+nXe8ZY8F7Xt5Sfqbso+SDg4lgJuGYn8YNdIgvfF2JSWY86GElAzsq+ayDKQvKHB+RRRjEkTEzMItsKTZ0JPjcZXdcjE+5BAfRvzbcYvJqkQI=;iY9rlsHtCO5GrVWbs9MNIg=="
---

