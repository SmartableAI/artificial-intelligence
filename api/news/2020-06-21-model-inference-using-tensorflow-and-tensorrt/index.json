{"path":"_news/2020-06-21-model-inference-using-tensorflow-and-tensorrt.md","title":"Model inference using TensorFlow and TensorRT","excerpt":"NVIDIA TensorRT is a high-performance inference optimizer and runtime that delivers low latency and high-throughput for deep learning inference applications. The following notebook demonstrates our recommended inference workflow.","sourceUrl":"https://smartableai.github.io/artificial-intelligence/api/news/2020-06-21-model-inference-using-tensorflow-and-tensorrt/index.json","webUrl":"https://docs.microsoft.com/en-us/azure/databricks/applications/deep-learning/inference/resnet-model-inference-tensorrt","originalUrl":null,"featuredContent":null,"heat":null,"tags":null,"images":[{"url":"https://docs.microsoft.com/en-us/media/logos/logo-ms-social.png","width":400,"height":400,"title":"Model inference using TensorFlow and TensorRT","attribution":null,"isCached":true}],"type":"article","ampWebUrl":null,"cdnAmpWebUrl":null,"publishedDateTime":"2020-06-20T21:56:00-07:00","updatedDateTime":null,"provider":{"name":"Microsoft","domain":"microsoft.com","images":[{"url":"https://smartableai.github.io/artificial-intelligence/assets/images/organizations/microsoft.com-50x50.jpg","width":50,"height":50,"title":null,"attribution":null,"isCached":false}],"publishers":null,"authors":null},"locale":"en-us","categories":["news"],"topics":["Google AI","AI","TensorFlow"]}